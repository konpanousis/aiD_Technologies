name: aid_Demo
data:
    data_path: ToTranslate/
    version: aid_Demo
    sgn: sign 
    txt: text
    gls: gloss
    train: alphapose-results.json
    dev:   alphapose-results.json
    test:  alphapose-results.json
    feature_size: 408
    level: word
    txt_lowercase: true
    max_sent_length: 800
    random_train_subset: -1
    random_dev_subset: -1
    gls_vocab: SLTModel/TranslationModel/data/glossAiD.vocab
    txt_vocab: SLTModel/TranslationModel/data/txtAiD.vocab
    batch_size: 32
testing:
    translation_beam_sizes:
    - 1
    translation_beam_alphas:
    - 1
training:
    load_model: "PreTrained/PlayGround2_lwta2_02/19840.ckpt" 
    reset_best_ckpt: true
    reset_scheduler: true
    reset_optimizer: true
    random_seed: 44
    model_dir:  "PreTrained/PlayGround2_lwta2_02" 
    recognition_loss_weight: 0
    translation_loss_weight: 1.0
    kl_weight: 0.2
    eval_metric: bleu
    optimizer: adam
    learning_rate: 0.001
    batch_size: 32
    eval_batch_size: 32
    num_valid_log: 5
    epochs: 500
    early_stopping_metric: eval_metric
    batch_type: sentence
    translation_normalization: batch
    eval_recognition_beam_size: 1
    eval_translation_beam_size: 1
    eval_translation_beam_alpha: 0
    overwrite: true
    shuffle: true
    use_cuda: true
    translation_max_output_length: 15
    keep_last_ckpts: 1
    batch_multiplier: 1
    logging_freq: 20
    validation_freq: 160
    betas:
    - 0.9
    - 0.998
    scheduling: plateau
    learning_rate_min: 0.00001
    patience: 3
    decrease_factor: 0.8
    label_smoothing: 0.0
model:
    gloss_input: false
    initializer: xavier
    bias_initializer: zeros
    init_gain: 1.0
    embed_initializer: xavier
    embed_init_gain: 1.0
    tied_softmax: false
    simplified_inference: true
    inference_sample_size: 4
    encoder:
        skip_encoder: false
        type: transformer
        bayesian_attention: True
        bayesian_feedforward: True
        ibp: false
        activation: lwta
        lwta_competitors: 2
        num_layers: 2
        num_heads: 8
        embeddings:
            embedding_dim: 256
            scale: false
            bayesian: True
            ibp: false
            dropout: 0.1
            norm_type: batch
            activation_type: lwta
            lwta_competitors: 2
        hidden_size: 256
        ff_size: 1024
        dropout: 0.1
    decoder:
        type: transformer
        num_layers: 2
        num_heads: 8
        bayesian_attention: True
        bayesian_feedforward: True
        bayesian_output: True
        ibp: false
        activation: lwta
        lwta_competitors: 2
        embeddings:
            embedding_dim: 256
            scale: False
            bayesian: False
            dropout: 0.1
            norm_type: batch
        hidden_size: 256
        ff_size: 1024
        dropout: 0.1
